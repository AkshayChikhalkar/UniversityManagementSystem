
\documentclass[conference,onecolumn]{IEEEtran}
%\documentclass[article,onecolumn]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{color}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\definecolor{yellow}{rgb}{0.85, 1,1}
\definecolor{pastleyellow}{rgb}{1, 0.98,0.63}
\setlength{\arrayrulewidth}{0.1mm}
\setlength{\tabcolsep}{6pt}
\setlength\headheight{10pt}
\usepackage{wrapfig}


\setlength\parskip{1em plus 0.1em minus 0.2em}
\setlength\parindent{0pt}
\setlength{\parskip}{8pt}
\usepackage{subcaption}
%\renewcommand{\arraystretch}{1.5}


\usepackage{tikz}
\usepackage{float}
\usepackage[linesnumbered, ruled,boxed]{algorithm2e}
\usetikzlibrary{trees}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees}

\definecolor{yellow}{rgb}{0.85, 1,1}
\definecolor{pastleyellow}{rgb}{1, 0.98,0.63}
\setlength{\arrayrulewidth}{0.2mm}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.5}
\tikzset{
	basic/.style= {draw, text width=2cm, rectangle},
	root/.style = {basic, rounded corners=2pt, thin, align=center},
	level 2/.style = {basic, rounded corners=6pt, thin,align=center,text width=8em},
	level 3/.style = {basic, very thick, rounded corners=3pt, thin,align=center,text width=8em},
	level 4/.style = {basic, thin, align=left, text width=6.5em},
	round/.style = {basic, thin,ellipse}
}


\begin{document}

\title{Performance Comparison: Exploring Dimensionality Reduction and Hyperparameter Tuning in GPU Classification}

\author{\IEEEauthorblockN{Akshay Chikhalkar}\\
\IEEEauthorblockA{\textit{Department of Electrical Engineering and Computer Science} \\
\textit{Technische Hochschule Ostwestfalen-Lippe University of Applied Sciences and Arts}\\
Lemgo, Germany \\
akshay.chikhalkar@stud.th-owl.de}

}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
classifier, model, Random Forest Classifier (RFC), Decision Tree Classifier (DTC), Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Gaussian Naive Bayes (GNB), Graphics processing unit (GPU), machine learning (ML)
\end{IEEEkeywords}

\newpage
\tableofcontents

\newpage
\section{Introduction}
fake citation \cite{C0}
Classification is a powerful technique that allows for faster and more efficient processing of large amounts of data. For technology experts, the ability to quickly and accurately classify data can open up new possibilities for research and experimentation. With the increasing amount of data being generated by devices and applications, the ability to process this data in real-time is becoming increasingly important. GPU classification allows for the creation of more sophisticated models and algorithms, enabling new insights and discoveries. Additionally, the use of GPU classification can significantly reduce the time and resources required for data processing, allowing for more efficient use of resources and cost savings. Overall, GPU classification is a valuable tool for anyone looking to push the boundaries of what is possible with data analysis and machine learning.

The current study was motivated by the desire to address the challenge of providing technology recommendations based on multiple factors. I noticed that family members, friends and colleagues often sought advice on technology products, particularly in the realm of computer technology. The complexity of the technology domain, as well as the increasing number of products being released, makes it difficult to keep track of all options and determine the best fit for individual needs.

To address this challenge, I proposed a classification solution that utilizes computer processing to classify technology products based on relevant features. The initial focus was the classification of the Graphics Processing Units based on memory type. However, the goal is to not only classify GPUs but also to expand this solution to other technology products.

The study aimed to copmare the performance of six classification algorithms and implement dimensionality reduction and hyperparameter optimization to further improve their performance for GPU classification based on release year, as release year plays a crucial role in the performance of a GPU and it's evolution. Each year have different characteristics and performance improvement which can impact the overall performance of a GPU. Six Machine Learning algorithms were employed and the script was written in Python programming language. By classifying GPUs based on release year, the study aimed to provide a more accurate and comprehensive evaluation of the products available in the market.

\subsection{Dtaset decsription}
    The dataset used in this study was obtained from Kaggle\footnote{https://www.kaggle.com/code/yukihm/data-mining-undip}. It contains information about 2889 GPUs from 2010 to 2017. The dataset contains 16 features, including the manufacturer, product name, release year, memory size, memory bus width, GPU clock speed, memory clock speed, texture mapping units (TMUs), raster operations pipelines (ROPs), pixel shader details, vertex shader details, integrated graphics processor (IGP) presence, data communication bus type, memory type and GPU chip information. The dataset was downloaded in CSV format and imported into Python for further analysis. The dataset was then split into training and testing sets, with 80\% of the data used for training and 20\% for testing. 
    This dataset is used to address the problem of GPU classification, where the goal is to categorise GPUs into different classes or categories based on their specifications. This categories includes perfirmance levels, memory capacity, clock speed and other technical attributes. This type of classification can be valuable for technology experts who are looking to make recommendations for technology products. It can also be useful for consumers who are looking to purchase a new GPU and want to compare different options based on their specifications. For industries, it can be helpful for manufacturers and retailers who are looking to classify their products and make recommendations for customers. E-commerce platforms can also use this type of classification to recommend compatible GPUs to users based on their requirements and budget. Further, game optimisation, market analysis, product development and other applications can benefit from this type of classification.

    \figurename~\ref{fig:datacorrelationmatrix} shows the class diagram of the dataset.
    
    appendix~\ref{appdx:cmheatmapdtc} shows the chatbot conversation with GPT-3.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{Plots/DataCorelation.png}
		\caption{Data correlation matrix of the dataset}
		\label{fig:datacorrelationmatrix}
	\end{figure}

    % \begin{wrapfigure}{r}{0.5\textwidth}
    %     \centering
    %     \includegraphics[width=0.5\textwidth]{Plots/DataCorelation.png}
    %     \caption{Data correlation matrix of the dataset}
    %     \label{fig:datacorrelationmatrix}
    % \end{wrapfigure}

\subsection{Data processing}
    First step towards data processing was involves addressing missing or incomplete data points that could reduce the purity of the dataset. The dataset was checked for missing values and out of 2889 data samples it was found that the dataset contained 1054 missing values in multiple columns. The missing values were replaced with approximated value in the column using interpolation technique. The dataset was then checked for duplicate values and it was found that the dataset contained 7 duplicate values. The duplicate values were removed from the dataset to ensure that the dataset was clean and ready for further analysis. The dataset was then checked using Z-score methode for outliers and it was found that the dataset contained 53 outliers with Z-score of 3 in the memory size column.
    After data processing, out of initial 2889 data samples only 1775 data samples were used for further analysis.
\input{State of the art.tex}

\section{Increasing classification performance}
    One of the method to increase the performance of the classifiers is to reduce the dimensionality of the dataset.
    \subsection{Dimensionality reduction}
        Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. Feature selection is the process of selecting a subset of relevant features for use in model construction. Feature extraction is the process of transforming data from a high-dimensional space into a space of fewer dimensions. The goal of dimensionality reduction is to reduce the number of random variables under consideration by obtaining a set of principal variables.
        Feature selection was chosen instead of feature extraction because feature extraction is a more complex process that requires more computational resources and time. Feature selection is a simpler process that can be performed quickly and easily. It also allows for more control over the features that are selected, which can be useful for certain applications. For example, if the goal is to classify GPUs based on their release year, then it would be more useful to select features that are related to the release year rather than features that are not related to the release year. This would allow for a more accurate classification of GPUs based on their release year.

    \subsubsection{Feature selection}
        Feature selection is the process of selecting a subset of relevant features for use in model construction. This techniques are used for several reasons:
        \begin{itemize}
            \item Simplification of models to make them easier to interpret by researchers/users,
            \item Shorter training times,
            \item To avoid the curse of dimensionality,
            \item Enhanced generalization by reducing overfitting (formally, reduction of variance)
        \end{itemize}

        In this project, the feature selection process was performed using the \emph{SelectKBest} function from the \emph{sklearn.feature\_selection} library. The \emph{SelectKBest} function selects the best features based on univariate statistical tests. The \emph{f\_classif} function was used to compute the ANOVA F-value for the classification task. The \emph{SelectKBest} function was used to select the top 5 features based on the F-value. The top 5 features were then used to train the classifiers and evaluate their performance. The feature selection process was performed for each classifier individually. The results of the feature selection process are shown in Table \ref{table:Feature selection results}. 
        
        The results show that the feature selection process improved the performance of all classifiers except for the Random Forest Classifier (RFC). The performance of the RFC decreased by 0.1\% after the feature selection process. The performance of the other classifiers increased by 0.1\% to 0.3\% after the feature selection process. The results also show that the feature selection process improved the performance of the classifiers by reducing the number of features from 16 to 5. This reduction in the number of features resulted in a reduction in the complexity of the classifiers, which in turn resulted in a reduction in the execution time of the classifiers. The execution time of the classifiers was reduced by 0.1\% to 0.3\% after the feature selection process. The results of the feature selection process are shown in Table \ref{table:Feature selection results}. To avoid any external affects such as multithread execution; on the performance of the classifiers, the feature selection process was performed multiple times and classifier was executed over 2000 times.

\section{Advanced classification methods}

    \subsection{Hyperparameter optimisation}

\section{Reduction complexity and execution time}
\section{Conclusion}













\newpage
\appendix

    \begin{table}[H]	
        \begin{center}
            \begin{tabular}[H]{ |m{1cm}|m{1cm}|m{1.2cm}|m{1.2cm}|m{1cm}|m{2cm}|m{3cm}|m{2cm}|}
                \hline
                \textbf{Item}&\textbf{Mean} &\textbf{Variance} &\textbf{Std. Dev.}  &\textbf{No.}  &\textbf{Left} &\textbf{Right} &\textbf{Scale}\\ \hline
                1	&1,8	&0,2	&0,4	&5	&annoying	            &enjoyable	                &Attractiveness             \\ \hline
                2	&2,2	&0,2	&0,4	&5	&not                    &understandable	            &understandable         \\ \hline
                3	&0,6	&2,3	&1,5	&5	&creative	            &dull	                    &Novelty        \\ \hline
                4	&3,0	&0,0	&0,0	&5	&easy to learn	        &difficult to learn	        &Perspicuity        \\ \hline
                5	&2,2	&0,2	&0,4	&5	&valuable	            &inferior	                &Stimulation        \\ \hline
                6	&0,8	&1,7	&1,3	&5	&boring	                &exciting	                &Stimulation        \\ \hline
                7	&1,8	&0,7	&0,8	&5	&not interesting	    &interesting	            &Stimulation        \\ \hline
                8	&2,2	&1,2	&1,1	&5	&unpredictable	        &predictable	            &Dependability      \\ \hline
                9	&2,2	&1,7	&1,3	&5	&fast	                &slow	                    &Efficiency     \\ \hline
                10	&-1,4	&2,3	&1,5	&5	&inventive	            &conventional	            &Novelty        \\ \hline
                11	&2,4	&0,3	&0,5	&5	&obstructive	        &supportive	                &Dependability      \\ \hline
                12	&2,2	&0,7	&0,8	&5	&good	                &bad	                    &Attractiveness     \\ \hline
                13	&1,2	&5,7	&2,4	&5	&complicated	        &easy	                    &Perspicuity        \\ \hline
                14	&2,0	&1,0	&1,0	&5	&unlikable	            &pleasing	                &Attractiveness     \\ \hline
                15	&-1,2	&2,7	&1,6	&5	&usual	                &leading edge	            &Novelty        \\ \hline
                16	&2,0	&0,5	&0,7	&5	&unpleasant	            &pleasant	                &Attractiveness     \\ \hline
                17	&2,4	&0,8	&0,9	&5	&secure	                &not secure	                &Dependability      \\ \hline
                18	&1,8	&0,2	&0,4	&5	&motivating	            &demotivating	            &Stimulation        \\ \hline
                19	&2,2	&1,7	&1,3	&5	&meets			        &does not meet				&Dependability      \\ \hline
                20	&2,6	&0,3	&0,5	&5	&inefficient	        &efficient	                &Efficiency     \\ \hline
                21	&1,8	&0,7	&0,8	&5	&clear	                &confusing	                &Perspicuity        \\ \hline
                22	&2,2	&0,7	&0,8	&5	&impractical	        &practical	                &Efficiency     \\ \hline
                23	&2,0	&1,0	&1,0	&5	&organized	            &cluttered	                &Efficiency     \\ \hline
                24	&2,0	&1,5	&1,2	&5	&attractive	            &unattractive	            &Attractiveness     \\ \hline
                25	&2,2	&0,7	&0,8	&5	&friendly	            &unfriendly	                &Attractiveness     \\ \hline
                26	&-1,4	&2,3	&1,5	&5	&conservative	        &innovative	                &Novelty        \\ 
                \hline
            \end{tabular}
        \end{center}
        \caption{Mean value per item}
        \label{table:Mean value per item}
    \end{table}

    \begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Plots/CM_Heatmap_DTC.png}
		\caption{Domain model of the University Room Management System}
		\label{appdx:cmheatmapdtc}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Plots/CM_Heatmap_GNB.png}
		\caption{Domain model of the University Room Management System}
		\label{appx:cmheatmapgnb}
	\end{figure}
    \begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Plots/CM_Heatmap_KNN.png}
		\caption{Domain model of the University Room Management System}
		\label{appx:cmheatmapknn}
	\end{figure}
    \begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Plots/CM_Heatmap_LDA.png}
		\caption{Domain model of the University Room Management System}
		\label{appx:cmheatmaplda}
	\end{figure}
    \begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Plots/CM_Heatmap_RFC.png}
		\caption{Domain model of the University Room Management System}
		\label{appx:cmheatmaprfc}
	\end{figure}
    \begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Plots/CM_Heatmap_SVM.png}
		\caption{Domain model of the University Room Management System}
		\label{appx:cmheatmapsvm}
	\end{figure}

    \begin{tabular}{lrrrrrrr}
        \hline
         Algorithm   &   CV\_Accuracy &   Accuracy &   Precision &   Recall &       F1 &   Execution Time &   Iterations \\
        \hline
         RFC         &      0.440845 &   0.473239 &    0.482281 & 0.473239 & 0.467693 &       0.172644   &            0 \\
         DTC         &      0.400704 &   0.442254 &    0.449104 & 0.442254 & 0.435644 &       0.00341725 &            0 \\
         SVM         &      0.280986 &   0.28169  &    0.253084 & 0.28169  & 0.219221 &       0.157703   &            0 \\
         KNN         &      0.360563 &   0.357746 &    0.379279 & 0.357746 & 0.351769 &       0.00796819 &            0 \\
         LDA         &      0.261972 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &       0.00258398 &            0 \\
         GNB         &      0.228873 &   0.205634 &    0.182054 & 0.205634 & 0.18264  &       0.00265884 &            0 \\
         RFC         &      0.440845 &   0.473239 &    0.482281 & 0.473239 & 0.467693 &       0.170908   &            1 \\
         DTC         &      0.400704 &   0.442254 &    0.449104 & 0.442254 & 0.435644 &       0.00406098 &            1 \\
         SVM         &      0.280986 &   0.28169  &    0.253084 & 0.28169  & 0.219221 &       0.161214   &            1 \\
         KNN         &      0.360563 &   0.357746 &    0.379279 & 0.357746 & 0.351769 &       0.0083189  &            1 \\
         LDA         &      0.261972 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &       0.00339317 &            1 \\
         GNB         &      0.228873 &   0.205634 &    0.182054 & 0.205634 & 0.18264  &       0.00308394 &            1 \\
         RFC         &      0.440845 &   0.473239 &    0.482281 & 0.473239 & 0.467693 &       0.187191   &            2 \\
         DTC         &      0.400704 &   0.442254 &    0.449104 & 0.442254 & 0.435644 &       0.00370598 &            2 \\
         SVM         &      0.280986 &   0.28169  &    0.253084 & 0.28169  & 0.219221 &       0.16112    &            2 \\
         KNN         &      0.360563 &   0.357746 &    0.379279 & 0.357746 & 0.351769 &       0.00886011 &            2 \\
         LDA         &      0.261972 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &       0.00250292 &            2 \\
         GNB         &      0.228873 &   0.205634 &    0.182054 & 0.205634 & 0.18264  &       0.00361419 &            2 \\
        \hline
    \end{tabular}
        
    \begin{tabular}{lrrrrrr}
        \hline
         Algorithm   &   CV\_Accuracy &   Accuracy &   Precision &   Recall &       F1 &   Execution Time \\
        \hline
         RFC         &      0.407746 &   0.439437 &    0.45987  & 0.439437 & 0.437043 &        1.25214   \\
         DTC         &      0.371127 &   0.422535 &    0.435677 & 0.422535 & 0.422189 &        0.0277729 \\
         SVM         &      0.311972 &   0.309859 &    0.2939   & 0.309859 & 0.266544 &        0.665152  \\
         KNN         &      0.376761 &   0.36338  &    0.384735 & 0.36338  & 0.363729 &        0.0385151 \\
         LDA         &      0.261268 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &        0.0127187 \\
         GNB         &      0.273239 &   0.24507  &    0.224912 & 0.24507  & 0.221897 &        0.0145471 \\
         RFC         &      0.407746 &   0.439437 &    0.45987  & 0.439437 & 0.437043 &        1.25332   \\
         DTC         &      0.371127 &   0.422535 &    0.435677 & 0.422535 & 0.422189 &        0.0303597 \\
         SVM         &      0.311972 &   0.309859 &    0.2939   & 0.309859 & 0.266544 &        0.67257   \\
         KNN         &      0.376761 &   0.36338  &    0.384735 & 0.36338  & 0.363729 &        0.0449691 \\
         LDA         &      0.261268 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &        0.0141079 \\
         GNB         &      0.273239 &   0.24507  &    0.224912 & 0.24507  & 0.221897 &        0.01702   \\
         RFC         &      0.407746 &   0.439437 &    0.45987  & 0.439437 & 0.437043 &        1.36698   \\
         DTC         &      0.371127 &   0.422535 &    0.435677 & 0.422535 & 0.422189 &        0.0315924 \\
         SVM         &      0.311972 &   0.309859 &    0.2939   & 0.309859 & 0.266544 &        0.686508  \\
         KNN         &      0.376761 &   0.36338  &    0.384735 & 0.36338  & 0.363729 &        0.0410869 \\
         LDA         &      0.261268 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &        0.013967  \\
         GNB         &      0.273239 &   0.24507  &    0.224912 & 0.24507  & 0.221897 &        0.0185232 \\
        \hline
    \end{tabular}
        
    \begin{tabular}{lrrrrrr}
        \hline
         Algorithm   &   CV\_Accuracy &   Accuracy &   Precision &   Recall &       F1 &   Execution Time \\
        \hline
         RFC         &      0.423239 &   0.459155 &    0.487308 & 0.459155 & 0.456431 &       51.2526    \\
         DTC         &      0.378873 &   0.430986 &    0.444931 & 0.430986 & 0.428687 &        0.193149  \\
         SVM         &      0.361972 &   0.394366 &    0.368064 & 0.394366 & 0.363465 &        3.54028   \\
         KNN         &      0.417606 &   0.450704 &    0.465625 & 0.450704 & 0.445552 &        0.133158  \\
         LDA         &      0.261268 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &        0.0122411 \\
         GNB         &      0.273239 &   0.24507  &    0.224912 & 0.24507  & 0.221897 &        0.015172  \\
         RFC         &      0.423239 &   0.459155 &    0.487308 & 0.459155 & 0.456431 &       52.1694    \\
         DTC         &      0.378873 &   0.430986 &    0.444931 & 0.430986 & 0.428687 &        0.204587  \\
         SVM         &      0.361972 &   0.394366 &    0.368064 & 0.394366 & 0.363465 &        3.50349   \\
         KNN         &      0.417606 &   0.450704 &    0.465625 & 0.450704 & 0.445552 &        0.154317  \\
         LDA         &      0.261268 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &        0.0139561 \\
         GNB         &      0.273239 &   0.24507  &    0.224912 & 0.24507  & 0.221897 &        0.018435  \\
         RFC         &      0.423239 &   0.459155 &    0.487308 & 0.459155 & 0.456431 &       56.1888    \\
         DTC         &      0.378873 &   0.430986 &    0.444931 & 0.430986 & 0.428687 &        0.209001  \\
         SVM         &      0.361972 &   0.394366 &    0.368064 & 0.394366 & 0.363465 &        3.53441   \\
         KNN         &      0.417606 &   0.450704 &    0.465625 & 0.450704 & 0.445552 &        0.142963  \\
         LDA         &      0.261268 &   0.24507  &    0.205744 & 0.24507  & 0.202823 &        0.0146387 \\
         GNB         &      0.273239 &   0.24507  &    0.224912 & 0.24507  & 0.221897 &        0.0194271 \\
        \hline
    \end{tabular}
\newpage
\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}